{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-coordinator",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp              \n",
    "import optax                           \n",
    "import torch.utils.data as Data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sol import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3\n",
    "N = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------#\n",
    "# CONFIG\n",
    "#------------------------------------------------------------------#\n",
    "\n",
    "config = {\n",
    "    'data_dir': f'/data/akash/decay_turbulence_T{T}_N{N}/',\n",
    "    'work_dir': './checkpoints/',\n",
    "    'epochs': 400,\n",
    "    'batch_size': 2,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0,\n",
    "    'seed': 23,\n",
    "    'dtype': jnp.float32,\n",
    "    'solver_dtype': jnp.bfloat16,\n",
    "    'timespan': N\n",
    "}\n",
    "\n",
    "try:\n",
    "    assert(inner_steps == T)\n",
    "    print(\"Solver inner steps checked\")\n",
    "except NameError:\n",
    "    print(\"No solver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(config['seed'])\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "state = create_train_state(\n",
    "    init_rng, \n",
    "    config,\n",
    ")\n",
    "\n",
    "def to_fp16(t):\n",
    "    return jax.tree_map(lambda x: x.astype(jnp.float16) if x.dtype == jnp.float32 else x, t)\n",
    "\n",
    "state = restore_checkpoint(state, config['work_dir']+'1r2jaj79')\n",
    "\n",
    "if config['dtype'] == jnp.float16:\n",
    "    state.update_value('params', to_fp16(state.params))\n",
    "    \n",
    "state.update_value('examples_seen', 0)\n",
    "state.update_value('tx', optax.adamw(config['learning_rate'], config['weight_decay']))\n",
    "state.update_value('dynamic_scale', optim.DynamicScale())\n",
    "state.update_value('best_val_loss', float(\"inf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-investigator",
   "metadata": {},
   "source": [
    "# Warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain for a bit\n",
    "\n",
    "config['timespan'] = 2\n",
    "train_dataset = CFDDataset(config=config, frac=0.2)\n",
    "train_loader = Data.DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0, collate_fn=numpy_collate, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-hamilton",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (SP, DP) in enumerate(tqdm(train_loader)):\n",
    "    state, metrics, pred = train_step(state, SP, DP, timespan=config['timespan'])\n",
    "\n",
    "    if i>=0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-caribbean",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-shoot",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load correct dataset\n",
    "config['timespan'] = N\n",
    "train_dataset = CFDDataset(config=config, frac=1)\n",
    "test_dataset = CFDDataset(config=config, train=False, frac=1)\n",
    "train_loader = Data.DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0, collate_fn=numpy_collate, drop_last=True)\n",
    "test_loader = Data.DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=0, collate_fn=numpy_collate, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-socket",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "RUN_ID = None\n",
    "if RUN_ID:\n",
    "    run = wandb.init(id=RUN_ID, project=\"akash-ddp\", resume=\"must\")\n",
    "else:\n",
    "    run = wandb.init(project=\"akash-ddp\", config=config)\n",
    "    \n",
    "config['work_dir'] = f'./checkpoints/{run.id}'\n",
    "# if RUN_ID:\n",
    "#     state = restore_checkpoint(state, config['work_dir'])\n",
    "    \n",
    "\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    state, train_metrics, pred = train_epoch(state, train_loader, epoch, config)\n",
    "    test_metrics = eval_model(state, test_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-latter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(state, config['work_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-intro",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
